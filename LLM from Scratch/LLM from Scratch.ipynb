{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <font color='blue'>LLM from scratch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XQhwlGcdpydo"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text data\n",
    "texts = open('text.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello, how are you? I am Camila.\\n'\n",
      "'Hello, Camila, my name is Fernando. Nice to meet you.\\n'\n",
      "'Nice to meet you too. How are you today?\\n'\n",
      "'Great. My soccer team won the competition.\\n'\n",
      "'Wow, congratulations Fernando!\\n'\n",
      "'Thank you, Camila.\\n'\n",
      "'Shall we have pizza later to celebrate?\\n'\n",
      "'Sure. Do you recommend any restaurant, Camila?\\n'\n",
      "'Yes, a new restaurant opened, and they say the banana pizza is phenomenal.\\n'\n",
      "'Okay. Shall we meet at the restaurant at seven in the evening?\\n'\n",
      "'Sounds good. See you later then.'\\n'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Preprocessing and Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter special characters: '.', ',', '?', '!'\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', texts.lower()).split('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'hello how are you i am camila\\\\n'\", \"'hello camila my name is fernando nice to meet you\\\\n'\", \"'nice to meet you too how are you today\\\\n'\", \"'great my soccer team won the competition\\\\n'\", \"'wow congratulations fernando\\\\n'\", \"'thank you camila\\\\n'\", \"'shall we have pizza later to celebrate\\\\n'\", \"'sure do you recommend any restaurant camila\\\\n'\", \"'yes a new restaurant opened and they say the banana pizza is phenomenal\\\\n'\", \"'okay shall we meet at the restaurant at seven in the evening\\\\n'\", \"'sounds good see you later then'\\\\n'\", '']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the sentences into words and create a list of words\n",
    "word_list = list(set(\" \".join(sentences).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['any', 'congratulations', 'too', 'fernando', 'seven', 'the', 'am', \"phenomenal\\\\n'\", \"'shall\", \"'hello\", 'name', \"celebrate\\\\n'\", 'and', 'team', 'meet', \"fernando\\\\n'\", 'at', \"'sure\", \"then'\\\\n'\", 'new', 'soccer', 'how', 'won', \"'okay\", 'camila', \"'nice\", \"'thank\", 'restaurant', 'shall', 'good', 'do', 'have', 'opened', \"you\\\\n'\", 'i', 'to', 'pizza', 'recommend', 'they', 'in', 'see', 'nice', \"'great\", \"evening\\\\n'\", \"'yes\", 'are', 'later', 'you', 'my', \"today\\\\n'\", \"'sounds\", \"camila\\\\n'\", 'we', 'say', 'a', 'banana', \"'wow\", 'is', \"competition\\\\n'\"]\n"
     ]
    }
   ],
   "source": [
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the word dictionary with BERT's special tokens\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the words in the dictionary and create indices\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, 'any': 4, 'congratulations': 5, 'too': 6, 'fernando': 7, 'seven': 8, 'the': 9, 'am': 10, \"phenomenal\\\\n'\": 11, \"'shall\": 12, \"'hello\": 13, 'name': 14, \"celebrate\\\\n'\": 15, 'and': 16, 'team': 17, 'meet': 18, \"fernando\\\\n'\": 19, 'at': 20, \"'sure\": 21, \"then'\\\\n'\": 22, 'new': 23, 'soccer': 24, 'how': 25, 'won': 26, \"'okay\": 27, 'camila': 28, \"'nice\": 29, \"'thank\": 30, 'restaurant': 31, 'shall': 32, 'good': 33, 'do': 34, 'have': 35, 'opened': 36, \"you\\\\n'\": 37, 'i': 38, 'to': 39, 'pizza': 40, 'recommend': 41, 'they': 42, 'in': 43, 'see': 44, 'nice': 45, \"'great\": 46, \"evening\\\\n'\": 47, \"'yes\": 48, 'are': 49, 'later': 50, 'you': 51, 'my': 52, \"today\\\\n'\": 53, \"'sounds\": 54, \"camila\\\\n'\": 55, 'we': 56, 'say': 57, 'a': 58, 'banana': 59, \"'wow\": 60, 'is': 61, \"competition\\\\n'\": 62}\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the order and put indices as keys and words as values in the dictionary\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[PAD]',\n",
       " 1: '[CLS]',\n",
       " 2: '[SEP]',\n",
       " 3: '[MASK]',\n",
       " 4: 'any',\n",
       " 5: 'congratulations',\n",
       " 6: 'too',\n",
       " 7: 'fernando',\n",
       " 8: 'seven',\n",
       " 9: 'the',\n",
       " 10: 'am',\n",
       " 11: \"phenomenal\\\\n'\",\n",
       " 12: \"'shall\",\n",
       " 13: \"'hello\",\n",
       " 14: 'name',\n",
       " 15: \"celebrate\\\\n'\",\n",
       " 16: 'and',\n",
       " 17: 'team',\n",
       " 18: 'meet',\n",
       " 19: \"fernando\\\\n'\",\n",
       " 20: 'at',\n",
       " 21: \"'sure\",\n",
       " 22: \"then'\\\\n'\",\n",
       " 23: 'new',\n",
       " 24: 'soccer',\n",
       " 25: 'how',\n",
       " 26: 'won',\n",
       " 27: \"'okay\",\n",
       " 28: 'camila',\n",
       " 29: \"'nice\",\n",
       " 30: \"'thank\",\n",
       " 31: 'restaurant',\n",
       " 32: 'shall',\n",
       " 33: 'good',\n",
       " 34: 'do',\n",
       " 35: 'have',\n",
       " 36: 'opened',\n",
       " 37: \"you\\\\n'\",\n",
       " 38: 'i',\n",
       " 39: 'to',\n",
       " 40: 'pizza',\n",
       " 41: 'recommend',\n",
       " 42: 'they',\n",
       " 43: 'in',\n",
       " 44: 'see',\n",
       " 45: 'nice',\n",
       " 46: \"'great\",\n",
       " 47: \"evening\\\\n'\",\n",
       " 48: \"'yes\",\n",
       " 49: 'are',\n",
       " 50: 'later',\n",
       " 51: 'you',\n",
       " 52: 'my',\n",
       " 53: \"today\\\\n'\",\n",
       " 54: \"'sounds\",\n",
       " 55: \"camila\\\\n'\",\n",
       " 56: 'we',\n",
       " 57: 'say',\n",
       " 58: 'a',\n",
       " 59: 'banana',\n",
       " 60: \"'wow\",\n",
       " 61: 'is',\n",
       " 62: \"competition\\\\n'\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "vocab_size = len(word_dict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for tokens\n",
    "token_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dZXQM7TIpydr"
   },
   "outputs": [],
   "source": [
    "# Loop through the sentences to create the list of tokens\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tab7hDxUpyds",
    "outputId": "d95b8e7c-85a3-4c3e-9c89-68525c297b71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 25, 49, 51, 38, 10, 55],\n",
       " [13, 28, 52, 14, 61, 7, 45, 39, 18, 37],\n",
       " [29, 39, 18, 51, 6, 25, 49, 51, 53],\n",
       " [46, 52, 24, 17, 26, 9, 62],\n",
       " [60, 5, 19],\n",
       " [30, 51, 55],\n",
       " [12, 56, 35, 40, 50, 39, 15],\n",
       " [21, 34, 51, 41, 4, 31, 55],\n",
       " [48, 58, 23, 31, 36, 16, 42, 57, 9, 59, 40, 61, 11],\n",
       " [27, 32, 56, 18, 20, 9, 31, 20, 8, 43, 9, 47],\n",
       " [54, 33, 44, 51, 50, 22],\n",
       " []]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Hello, how are you? I am Camila.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First phrase\n",
    "texts[0:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 25, 49, 51, 38, 10, 55]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First sentence in token format (to be used for training the BERT model)\n",
    "token_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_HzEd-kcpydt"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 6\n",
    "n_segments = 2\n",
    "dropout = 0.2\n",
    "\n",
    "# Maximum length\n",
    "maxlen = 100\n",
    "\n",
    "# Maximum number of tokens to predict\n",
    "max_pred = 7\n",
    "\n",
    "# Number of layers\n",
    "n_layers = 6\n",
    "\n",
    "# Number of heads in multi-head attention\n",
    "n_heads = 12\n",
    "\n",
    "# Embedding size\n",
    "d_model = 768\n",
    "\n",
    "# Dimension of feedforward layer: 4 * d_model\n",
    "d_ff = d_model * 4\n",
    "\n",
    "# Dimension of K(=Q)V\n",
    "d_k = d_v = 64\n",
    "\n",
    "# Epochs\n",
    "NUM_EPOCHS = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Data Batches and Application of Special Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `make_batch()` below creates batches of data for training the BERT model. It is responsible for generating the correct input required for BERT training, which includes input tokens, masked tokens, masked token positions, segment IDs, and a label indicating whether the second sentence immediately follows the first. Let's describe each part of the function and use images to aid understanding.\n",
    "\n",
    "**Initialization**: The function starts by initializing an empty batch and counters for positive and negative sentences. Positive sentences are pairs where the second sentence immediately follows the first, while negative sentences are pairs where this does not occur. The batch must be balanced between positive and negative sentences.\n",
    "\n",
    "**Generating Sentence Pairs**: For each instance in the batch, the function randomly selects two sentences from the dataset. Each sentence is then converted into a list of token IDs, and special tokens `[CLS]` and `[SEP]` are added at appropriate positions.\n",
    "\n",
    "**Segment IDs**: For each sentence pair, the function generates segment IDs, where IDs are 0 for tokens in the first sentence and 1 for tokens in the second sentence.\n",
    "\n",
    "**Masked Language Model (MLM)**: The function then randomly selects 15% of the tokens to mask for the MLM task, ensuring that `[CLS]` and `[SEP]` tokens are not masked. These tokens are replaced by the `[MASK]` token, a random token, or remain unchanged, depending on a random draw.\n",
    "\n",
    "**Padding**: The function pads input IDs, segment IDs, masked tokens, and masked positions to ensure all lists have the same length.\n",
    "\n",
    "**Next Sentence Prediction**: Finally, the function checks if the second sentence immediately follows the first. If yes, it adds a `True` label to the instance and increments the positive counter. If not, it adds a `False` label and increments the negative counter.\n",
    "\n",
    "This function continues generating instances until the batch is full and contains an equal number of positive and negative instances. The batch is then returned.\n",
    "\n",
    "Note that this function serves as an example of how data can be prepared for BERT training. Depending on the dataset and specific task, adjustments to this function may be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fYLFtroMpydu"
   },
   "outputs": [],
   "source": [
    "# Define the function to create data batches\n",
    "def make_batch():\n",
    "    \n",
    "    # Initialize the batch as an empty list\n",
    "    batch = []\n",
    "    \n",
    "    # Initialize counters for positive and negative examples\n",
    "    positive = negative = 0\n",
    "    \n",
    "    # Continue until half of the batch is positive examples and the other half is negative examples\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        \n",
    "        # Choose random indices for two sentences\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        \n",
    "        # Retrieve tokens corresponding to the indices\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        \n",
    "        # Prepare input ids by adding special tokens [CLS] and [SEP]\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        \n",
    "        # Define segment ids to differentiate the two sentences\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "        # Calculate the number of predictions to be made (15% of tokens)\n",
    "        n_pred = min(max_pred, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        \n",
    "        # Identify candidate positions for masking that are not [CLS] or [SEP]\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        \n",
    "        # Shuffle candidate positions\n",
    "        shuffle(cand_maked_pos)\n",
    "        \n",
    "        # Initialize lists for masked tokens and their positions\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        \n",
    "        # Mask tokens until reaching the desired number of predictions\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            \n",
    "            # Random mask\n",
    "            if random() < 0.8:  \n",
    "                input_ids[pos] = word_dict['[MASK]'] \n",
    "            \n",
    "            # Replace with another token 10% of the time (20% of the remaining time)\n",
    "            elif random() < 0.5:  \n",
    "                index = randint(0, vocab_size - 1) \n",
    "                input_ids[pos] = word_dict[number_dict[index]] \n",
    "        \n",
    "        # Add zero padding to input ids and segment ids to reach maximum length\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        \n",
    "        # Add zero padding to masked tokens and their positions if necessary\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "        \n",
    "        # Add to the batch as a positive example if sentences are consecutive\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) \n",
    "            positive += 1\n",
    "        \n",
    "        # Add to the batch as a negative example if sentences are not consecutive\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) \n",
    "            negative += 1\n",
    "    \n",
    "    # Return the complete batch\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tun2vU1dpydv"
   },
   "outputs": [],
   "source": [
    "# Padding function\n",
    "def get_attn_pad_masked(seq_q, seq_k):\n",
    "    \n",
    "    batch_size, len_q = seq_q.size()\n",
    "    \n",
    "    batch_size, len_k = seq_k.size()\n",
    "    \n",
    "    pad_attn_masked = seq_k.data.eq(0).unsqueeze(1)\n",
    "    \n",
    "    return pad_attn_masked.expand(batch_size, len_q, len_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above creates an attention mask for padding tokens in a sequence.\n",
    "\n",
    "**Inputs**: The function takes two sequences, seq_q and seq_k. These are typically the query sequence and the key sequence in an attention operation.\n",
    "\n",
    "**Batch size extraction**: The function extracts the batch size and sequence lengths (len_q and len_k) from the input sequences' dimensions.\n",
    "\n",
    "**Mask creation**: The attention mask is created by checking which elements in seq_k are equal to zero (indicating a padding token). This results in a boolean matrix of the same size as seq_k, where True indicates a padding token and False indicates a real token.\n",
    "\n",
    "**Adding a dimension**: A dimension is added to the mask using the `unsqueeze(1)` method, which adds an extra dimension at index 1. This is necessary because the attention mask must match the dimensions of the attention matrices in the Transformer.\n",
    "\n",
    "**Mask expansion**: Finally, the mask is expanded to match the size of the attention matrix, which has dimensions (batch_size, len_q, len_k). The expanded mask is returned by the function.\n",
    "\n",
    "In summary, the function creates a mask that can be used to prevent the model from attending to padding tokens when computing attention. Padding tokens are used to pad sequences to equal lengths, but they carry no useful information, so it's important to ensure that the model ignores them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Fq2BFQ0kpydw"
   },
   "outputs": [],
   "source": [
    "# Create a batch\n",
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "l1sOb8tbpydx"
   },
   "outputs": [],
   "source": [
    "# Extract batch elements\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 27, 32, 56, 18, 20,  3, 31, 20,  8, 43,  9,  3,  2, 13, 25, 49, 51,\n",
       "         38, 10, 55,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3, 33, 44, 51, 50, 22,  2, 60,  5, 19,  2,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3,  3, 49, 51, 38, 10, 55,  2, 46, 52, 24, 17, 26,  9,  3,  2,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 29,  3, 18, 51,  6, 11, 49, 51, 53,  2,  3, 52, 24, 17, 26,  9, 62,\n",
       "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 30, 51, 55,  2,  3, 56, 35, 40, 50, 39,  3,  2,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 13, 28, 52, 14,  3,  7, 45, 39, 18, 37,  2, 29, 39,  3, 51,  6, 25,\n",
       "         49, 51, 53,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ids of the inputs\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 27, 32, 56, 18, 20,  3, 31, 20,  8, 43,  9,  3,  2, 13, 25, 49, 51,\n",
       "        38, 10, 55,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ids of first entry\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47,  9,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 12,  6,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isNext[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGWch4cvpydx",
    "outputId": "ef366521-5f38-41eb-c170-62de1cfa8c75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]),\n",
       " tensor([ 1, 27, 32, 56, 18, 20,  3, 31, 20,  8, 43,  9,  3,  2, 13, 25, 49, 51,\n",
       "         38, 10, 55,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply padding function\n",
    "get_attn_pad_masked(input_ids, input_ids)[0][0], input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "\n",
    "The image below is a high-level description of the Transformer encoder. The input is a sequence of tokens, which are first embedded into vectors and then processed in the neural network. The output is a sequence of vectors of size H, where each vector corresponds to an input token with the same index.\n",
    "\n",
    "Technically, predicting the output words requires:\n",
    "\n",
    "- 1- Adding a classification layer on top of the encoder output.\n",
    "- 2- Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n",
    "- 3- Calculating the probability of each word in the vocabulary with softmax.\n",
    "\n",
    "The loss function in the BERT model only considers the prediction of masked values and ignores the prediction of unmasked words. As a consequence, the model converges more slowly than directional models, a characteristic compensated by its greater contextual awareness.\n",
    "\n",
    "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair where the second sentence is the subsequent sentence in the original document, while in the other 50%, a random sentence from the corpus is chosen as the second sentence.\n",
    "\n",
    "To help the model distinguish between the two sentences during training, the input is processed as follows before entering the model:\n",
    "\n",
    "- 1- A [CLS] token is inserted at the beginning of the first sentence, and a [SEP] token is inserted at the end of each sentence.\n",
    "- 2- A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "- 3- A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of the positional embedding are presented in the Transformer paper.\n",
    "\n",
    "In fact, the embedding used to train the model is a combination of several embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeLU activation function\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Embedding Module\n",
    "\n",
    "The Embedding class below is part of the BERT architecture. Individual components of the class:\n",
    "\n",
    "**Initialization (`def __init__(self)`):** The class constructor initializes the necessary components for embeddings.\n",
    "\n",
    "- `self.tok_embed`: This is the token embedding layer that maps each token to a vector of dimension `d_model`.\n",
    "- `self.pos_embed`: This is the positional embedding layer that maps the position of a token within a sequence to a vector of dimension `d_model`.\n",
    "- `self.seg_embed`: This is the segment embedding layer that maps the type of token (0 for the first sentence and 1 for the second sentence) to a vector of dimension `d_model`.\n",
    "- `self.norm`: This is the layer normalization component used to normalize the embedding vectors.\n",
    "\n",
    "**Forward Method (`def forward(self, x, seg)`):** The forward method is where the actual embedding happens.\n",
    "\n",
    "- First, it calculates the position of each token in the sequence.\n",
    "- Next, it creates a position matrix of the same shape as the input `x` using `pos.unsqueeze(0).expand_as(x)`.\n",
    "- Then, it computes the total embedding as the sum of token, position, and segment embeddings.\n",
    "- Finally, it normalizes the embedding using the layer normalization and returns the result.\n",
    "\n",
    "The combination of these three embeddings allows BERT to consider both the individual meaning of the tokens and their order in the sequence, as well as whether the token belongs to the first or the second sentence. This makes the BERT embedding very powerful and flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Class\n",
    "class Embedding(nn.Module):\n",
    "    \n",
    "    # Constructor method\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        # Token embedding\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  \n",
    "        \n",
    "        # Position embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  \n",
    "        \n",
    "        # Segment (token type) embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  \n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Forward method\n",
    "    def forward(self, x, seg):\n",
    "        \n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        \n",
    "        # (seq_len,) -> (batch_size, seq_len)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  \n",
    "        \n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        \n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Scaled Dot Product Attention Module\n",
    "\n",
    "Below is the implementation of the Scaled Dot-Product Attention mechanism, which is a key part of the Transformer model used in BERT and other natural language processing models.\n",
    "\n",
    "Here is a line-by-line explanation of the forward method:\n",
    "\n",
    "**Scores**: The dot product of Q (query matrix) and K (key matrix) is calculated to determine the score for each key-query pair. These scores determine how much each element of the input sequence should be attended to in producing the output representation for a given element. The score is then scaled by the square root of the dimension of the keys (d_k) to prevent the dot product values from becoming too large in high-dimensional settings.\n",
    "\n",
    "**Attention Mask**: The attention mask is applied to the scores by filling the locations where the mask has a value of 1 with a very large negative number (-1e9). This ensures that these locations receive a weight close to zero when softmax is applied.\n",
    "\n",
    "**Softmax**: The softmax function is applied to the last axis of the scores to obtain the attention weights. This ensures that all weights are positive and sum to 1, so they can be interpreted as probabilities.\n",
    "\n",
    "**Context**: The attention weights are then multiplied by the value matrix V to obtain the output of the attention mechanism. Each value is weighted by the amount we should \"attend\" to that value, as determined by the attention weights.\n",
    "\n",
    "The method returns the context (the weighted output) and the attention matrix.\n",
    "\n",
    "In the Transformer model, Scaled Dot-Product Attention is used multiple times in each layer, allowing the model to attend to different parts of the input while producing each element of the output. This enables the Transformer to effectively handle long-range dependencies between words in input sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class to perform scaled dot-product attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \n",
    "    # Initialization method\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Initialize the base class\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    # Forward method to define the forward pass of the data\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \n",
    "        # Compute the attention scores as the product of Q and K, scaled by the key size\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "        \n",
    "        # Apply the attention mask to avoid attending to certain tokens\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        \n",
    "        # Apply softmax to obtain normalized attention weights\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        \n",
    "        # Multiply the attention weights by V to get the context\n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        # Return the context and attention weights\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Multi-Head Attention Module\n",
    "\n",
    "Below is the implementation of Multi-Head Attention, which is a key component of the Transformer architecture used in models like BERT. The idea of multi-head attention is to apply scaled dot-product attention multiple times in parallel, each with different learned weights. This allows the model to focus on different positions and capture various types of information.\n",
    "\n",
    "Let's analyze the forward method line by line:\n",
    "\n",
    "**Initialization**: `residual` and `batch_size` are initialized with `Q` and the size of the first axis of `Q`, respectively. The `residual` will be used later for the residual connection path.\n",
    "\n",
    "**Linear Transformations**: We apply linear transformations to the input data (Q, K, and V) using different weights. These transformations generate multiple \"heads\" of attention.\n",
    "\n",
    "**Reshaping**: The outputs of these linear transformations are then reshaped and transposed to have the appropriate form for scaled dot-product attention.\n",
    "\n",
    "**Attention Mask**: The attention mask is adjusted to match the format of the attention heads.\n",
    "\n",
    "**Scaled Dot-Product Attention**: Scaled dot-product attention is then applied to each of the attention heads.\n",
    "\n",
    "**Context Reshaping**: The output (context) from each attention head is then reshaped and concatenated.\n",
    "\n",
    "**Linear Transformation and Normalization**: A linear transformation is applied to the concatenated context, followed by layer normalization.\n",
    "\n",
    "**Residual Connection**: The final output is obtained by adding the output of the layer normalization to the residual connection path (original input Q).\n",
    "\n",
    "Finally, the function returns the normalized output and the attention matrix. Multi-head attention allows the model to consider information from different parts of the input sequence, in different representation subspaces, simultaneously, which enhances the model's ability to capture various features of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "FEMcW2aEpydy"
   },
   "outputs": [],
   "source": [
    "# Define the class to perform multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        # Initialize the base class\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # Define the weight matrix for queries Q\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        # Define the weight matrix for keys K\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        # Define the weight matrix for values V\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "    # Forward method to define the forward pass of the data\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \n",
    "        # Save the input Q for residual connection and get the batch size\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        \n",
    "        # Process Q through W_Q and reshape to have [n_heads] on the second dimension\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        \n",
    "        # Process K through W_K and reshape to have [n_heads] on the second dimension\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
    "        \n",
    "        # Process V through W_V and reshape to have [n_heads] on the second dimension\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
    "        \n",
    "        # Adjust attn_mask to be compatible with the dimensions of q_s, k_s, v_s\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        \n",
    "        # Calculate scaled dot-product attention and context for each attention head\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        \n",
    "        # Reshape context to combine the attention heads and return to the original format\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        \n",
    "        # Apply a linear transformation to the combined context\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        \n",
    "        # Normalize the output layer and add the residual\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Embedding object\n",
    "emb = Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Embeddings\n",
    "embeds = emb(input_ids, segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an attention mask\n",
    "attenM = get_attn_pad_masked(input_ids, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the MultiHeadAttention\n",
    "MHA = MultiHeadAttention()(embeds, embeds, embeds, attenM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYpEq8Kgpydy",
    "outputId": "fc0b2840-f834-4c29-dbcf-0a9b1d017438"
   },
   "outputs": [],
   "source": [
    "# Output\n",
    "output, A = MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0388, 0.0563, 0.0715,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0545, 0.0541, 0.0661,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0617, 0.0449, 0.0568,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0478, 0.0441, 0.0355,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0456, 0.0409, 0.0503,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0528, 0.0445, 0.0426,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Positional Feedforward Module\n",
    "\n",
    "This is the implementation of the Positional Feedforward Network (PoswiseFeedForward), which is a component of the Transformer architecture used in models like BERT.\n",
    "\n",
    "The Positional Feedforward Network consists of two linear layers with a GELU (Gaussian Error Linear Unit) activation in between.\n",
    "\n",
    "Here is a detailed explanation of the forward method:\n",
    "\n",
    "**First Linear Layer (self.fc1)**: The input x is passed through a linear layer (also known as a fully connected layer). This layer performs a linear transformation with d_model inputs and d_ff outputs, where d_model is the embedding space dimension and d_ff is the hidden layer dimension of the feedforward network. This allows the model to learn nonlinear representations.\n",
    "\n",
    "**GELU Activation**: Next, the GELU activation is applied. The GELU function allows the model to learn more complex and nonlinear transformations. It helps address the vanishing gradient problem by enabling more information to pass through the network.\n",
    "\n",
    "**Second Linear Layer (self.fc2)**: Finally, the output of the GELU activation is passed through a second linear layer, which transforms the output back to the original dimension d_model. This is done so that the output of this feedforward network can be added to the original input (residual connection) in the Transformer.\n",
    "\n",
    "The function returns, therefore, the output of this second linear layer, which has gone through the transformation of the first linear layer, GELU activation, and the second linear layer.\n",
    "\n",
    "Positional feedforward networks are an important part of Transformer models, allowing them to learn more complex representations and make nonlinear transformations of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for the Positional Feed Forward network\n",
    "class PoswiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        # Initialize the base class\n",
    "        super(PoswiseFeedForward, self).__init__()\n",
    "        \n",
    "        # First linear layer that increases the dimension of the data from d_model to d_ff\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # Second linear layer that reduces the dimension back from d_ff to d_model\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    # Forward method to define the forward pass of the data\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply the first linear transformation, followed by the GELU activation function \n",
    "        # and then the second linear transformation\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Encoder Layer Module\n",
    "\n",
    "This class defines an Encoder Layer, which is a component of the Transformer architecture and is also used in models like BERT. Each encoder layer in the Transformer contains two sub-layers: a Multi-Head Attention layer and a Positional Feed-Forward Network.\n",
    "\n",
    "Here is a detailed explanation of the forward method:\n",
    "\n",
    "**Multi-Head Attention (self.enc_self_attn)**: The input `enc_inputs` passes through a Multi-Head Attention layer, which allows each word in the input to attend to all other words. This layer also receives a mask (`enc_self_attn_mask`) used to prevent the model from attending to certain words (such as padding tokens). The output of the Multi-Head Attention is another sequence of vector representations with the same dimension as the input. The attention matrix, showing how each word attended to all the others, is also returned.\n",
    "\n",
    "**Positional Feed-Forward Network (self.pos_ffn)**: The output of the Multi-Head Attention layer is then passed through a Positional Feed-Forward Network. This is a simple neural network that operates independently at each position in the sequence (i.e., the same network is applied to each position). This allows the model to learn more complex representations and perform nonlinear transformations of the data.\n",
    "\n",
    "The function returns the output of this encoder layer, which is the output of the Positional Feed-Forward Network, along with the attention matrix. Thus, the input and output of this encoder layer have the same dimension, allowing multiple encoder layers to be stacked to form the complete Transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "f6Eij7fGpydz"
   },
   "outputs": [],
   "source": [
    "# Define the class for the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        # Initialize the base class\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Instantiate the multi-head attention for encoder self-attention\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        \n",
    "        # Instantiate the positional feed-forward network to use after self-attention\n",
    "        self.pos_ffn = PoswiseFeedForward()\n",
    "\n",
    "    # Forward method to define the forward pass of the data\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        \n",
    "        # Apply self-attention to the input data\n",
    "        enc_inputs, atnn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        \n",
    "        # After self-attention, pass the result through the positional feed-forward network\n",
    "        enc_inputs = self.pos_ffn(enc_inputs)\n",
    "        \n",
    "        # Return the output of the encoder and the attention weights\n",
    "        return enc_inputs, atnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Final Architecture of the LLM (BERT Model)\n",
    "\n",
    "This class defines the BERT (Bidirectional Encoder Representations from Transformers) model, a state-of-the-art language model that uses transformers and bidirectional attention to understand the semantics of words within context.\n",
    "\n",
    "Let's break down the forward method in detail:\n",
    "\n",
    "**Embedding (self.embedding)**: Transforms the inputs (`input_ids` and `segment_ids`) into dense vectors (embeddings).\n",
    "\n",
    "**Attention Mask (get_attn_pad_masked)**: Generates an attention mask to ignore padding tokens in the inputs.\n",
    "\n",
    "**Encoder Layers (self.layers)**: Passes the output from the embedding and the attention mask through multiple encoder layers. Each encoder layer consists of a multi-head attention layer and a positional feed-forward network.\n",
    "\n",
    "**Pooling (self.activ1(self.fc(output[:, 0]))): Applies a fully connected layer and a hyperbolic tangent activation function to the first position (the classification token) of each sequence in the encoder output. This results in a sequence representation vector.\n",
    "\n",
    "**Classifier (self.classifier)**: A fully connected layer that generates logits for the next-sentence classification task.\n",
    "\n",
    "**Masked Token Extraction (torch.gather(output, 1, masked_pos))**: Selects the output vectors corresponding to the masked tokens.\n",
    "\n",
    "**Masked Token Transformation (self.norm(self.activ2(self.linear(h_masked))))**: Applies a linear transformation, GELU activation, and normalization to the output of the masked tokens.\n",
    "\n",
    "**Decoder (self.decoder)**: A linear layer that generates logits for the masked language modeling task. It uses the same weights as the token embedding layer for consistency in the representation space. This decoder function is used solely to generate the final logits and is not used in the models training process.\n",
    "\n",
    "The method returns the logits for the masked language modeling task and the next-sentence classification task. These logits can then be used to calculate losses for both tasks during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "-hstWm_rpydz"
   },
   "outputs": [],
   "source": [
    "# BERT Model\n",
    "class BERT(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding()\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.activ1 = nn.Tanh()\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.activ2 = gelu\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        \n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        \n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        \n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        \n",
    "        self.decoder.weight = embed_weight\n",
    "        \n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        \n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        \n",
    "        enc_self_attn_mask = get_attn_pad_masked(input_ids, input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        \n",
    "        h_pooled = self.activ1(self.fc(output[:, 0]))\n",
    "        \n",
    "        logits_clsf = self.classifier(h_pooled)\n",
    "        \n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
    "        \n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "        \n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        \n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "        \n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliao do LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "modelo_dsa = BERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(modelo_dsa.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a breakdown of the typical training loop for one epoch in a machine learning model:\n",
    "\n",
    "**`optimizer.zero_grad()`**: Zeros the gradients of all optimized variables. This is necessary because gradients in PyTorch are accumulated, meaning each time `.backward()` is called, gradients are added to the existing ones rather than being replaced. Thus, we need to clear these accumulated gradients before each optimization step.\n",
    "\n",
    "**`logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)`**: Feeds the input data into the model and obtains the model's output. The output consists of `logits_lm` and `logits_clsf`, which are the raw, unnormalized results for the language modeling task and the classification task, respectively.\n",
    "\n",
    "**`loss_lm = criterion(logits_lm.transpose(1,2), masked_tokens)`**: Computes the loss for the masked language modeling task. `criterion` is the loss function, `logits_lm.transpose(1,2)` are the model's predictions, and `masked_tokens` are the true targets.\n",
    "\n",
    "**`loss_lm = (loss_lm.float()).mean()`**: Converts the loss to a floating-point type (if it isnt already) and then calculates the mean of the loss.\n",
    "\n",
    "**`loss_clsf = criterion(logits_clsf, isNext)`**: Computes the loss for the next sentence classification task.\n",
    "\n",
    "**`loss = loss_lm + loss_clsf`**: Combines the two losses into a single scalar loss.\n",
    "\n",
    "**`loss.backward()`**: Computes the gradients of all optimized variables with respect to the loss. These gradients are calculated with respect to the combined loss.\n",
    "\n",
    "**`optimizer.step()`**: Updates the model parameters using the computed gradients.\n",
    "\n",
    "These steps are repeated for each epoch of training. Each epoch represents a complete cycle through the training dataset. Therefore, if `NUM_EPOCHS` is 10, the entire training process is executed 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "OtgfwJ17pyd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss 85.6257\n",
      "Epoch: 2 | Loss 103.3067\n",
      "Epoch: 3 | Loss 374.3922\n",
      "Epoch: 4 | Loss 83.0070\n",
      "Epoch: 5 | Loss 127.3027\n",
      "Epoch: 6 | Loss 56.7147\n",
      "Epoch: 7 | Loss 77.9815\n",
      "Epoch: 8 | Loss 80.3260\n",
      "Epoch: 9 | Loss 66.5356\n",
      "Epoch: 10 | Loss 45.4595\n",
      "Epoch: 11 | Loss 31.6981\n",
      "Epoch: 12 | Loss 37.3686\n",
      "Epoch: 13 | Loss 37.8565\n",
      "Epoch: 14 | Loss 35.4382\n",
      "Epoch: 15 | Loss 39.5302\n",
      "Epoch: 16 | Loss 37.2350\n",
      "Epoch: 17 | Loss 32.1279\n",
      "Epoch: 18 | Loss 33.0896\n",
      "Epoch: 19 | Loss 32.7537\n",
      "Epoch: 20 | Loss 33.2081\n",
      "Epoch: 21 | Loss 33.5253\n",
      "Epoch: 22 | Loss 34.1890\n",
      "Epoch: 23 | Loss 32.7589\n",
      "Epoch: 24 | Loss 31.0161\n",
      "Epoch: 25 | Loss 28.2764\n",
      "Epoch: 26 | Loss 26.2311\n",
      "Epoch: 27 | Loss 29.8080\n",
      "Epoch: 28 | Loss 29.9723\n",
      "Epoch: 29 | Loss 26.2666\n",
      "Epoch: 30 | Loss 21.8840\n",
      "Epoch: 31 | Loss 22.9788\n",
      "Epoch: 32 | Loss 22.6751\n",
      "Epoch: 33 | Loss 21.4630\n",
      "Epoch: 34 | Loss 21.1502\n",
      "Epoch: 35 | Loss 20.3271\n",
      "Epoch: 36 | Loss 20.1865\n",
      "Epoch: 37 | Loss 19.4822\n",
      "Epoch: 38 | Loss 20.1860\n",
      "Epoch: 39 | Loss 18.7797\n",
      "Epoch: 40 | Loss 18.2321\n",
      "Epoch: 41 | Loss 17.7662\n",
      "Epoch: 42 | Loss 17.1350\n",
      "Epoch: 43 | Loss 17.2484\n",
      "Epoch: 44 | Loss 16.3321\n",
      "Epoch: 45 | Loss 14.7057\n",
      "Epoch: 46 | Loss 15.2253\n",
      "Epoch: 47 | Loss 15.1343\n",
      "Epoch: 48 | Loss 15.0261\n",
      "Epoch: 49 | Loss 14.7562\n",
      "Epoch: 50 | Loss 13.8835\n",
      "CPU times: total: 11min 35s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Start the training loop for a defined number of epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # Zero the gradients of the optimizer to prevent accumulation from previous epochs\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Pass the input data through the model and get logits for language masking \n",
    "    # and next sentence classification\n",
    "    logits_lm, logits_clsf = modelo_dsa(input_ids, segment_ids, masked_pos)\n",
    "    \n",
    "    # Compute the loss for the language masking task by comparing the predicted logits \n",
    "    # with the actual masked tokens\n",
    "    loss_lm = criterion(logits_lm.transpose(1,2), masked_tokens)\n",
    "    \n",
    "    # Compute the mean of the loss for normalization\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    \n",
    "    # Compute the loss for the next sentence classification task\n",
    "    loss_clsf = criterion(logits_clsf, isNext)\n",
    "    \n",
    "    # Combine the losses from both tasks to get the total loss\n",
    "    loss = loss_lm + loss_clsf\n",
    "    \n",
    "    # Print the current epoch and total loss\n",
    "    print(f'Epoch: {epoch + 1} | Loss {loss:.4f}')\n",
    "    \n",
    "    # Perform backpropagation to compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the model parameters based on the calculated gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Predictions from the Trained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello, how are you? I am Camila.\\n'\n",
      "'Hello, Camila, my name is Fernando. Nice to meet you.\\n'\n",
      "'Nice to meet you too. How are you today?\\n'\n",
      "'Great. My soccer team won the competition.\\n'\n",
      "'Wow, congratulations Fernando!\\n'\n",
      "'Thank you, Camila.\\n'\n",
      "'Shall we have pizza later to celebrate?\\n'\n",
      "'Sure. Do you recommend any restaurant, Camila?\\n'\n",
      "'Yes, a new restaurant opened, and they say the banana pizza is phenomenal.\\n'\n",
      "'Okay. Shall we meet at the restaurant at seven in the evening?\\n'\n",
      "'Sounds good. See you later then.'\\n'\n",
      "\n",
      "['[CLS]', \"'hello\", 'camila', '[MASK]', 'name', 'is', 'fernando', 'nice', 'to', 'meet', \"you\\\\n'\", '[SEP]', \"'sure\", 'do', 'you', 'recommend', '[MASK]', 'restaurant', \"camila\\\\n'\", '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Extrai o batch\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(texts)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Masket Tokens List:  [13, 52, 4]\n",
      "Predicted Masked TOkens List:  []\n"
     ]
    }
   ],
   "source": [
    "# Extract Token Predictions\n",
    "logits_lm, logits_clsf = modelo_dsa(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('Real Masket Tokens List: ', [pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('Predicted Masked Tokens List: ', [pos for pos in logits_lm if pos != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsAhgTZCpyd0",
    "outputId": "ccd4cae3-835d-4678-da37-41018d77354f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isNext (Real Value):  False\n",
      "isNext (Predicted Value):  False\n"
     ]
    }
   ],
   "source": [
    "# Extract the predictons of the next token\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext (Real Value): ', True if isNext else False)\n",
    "print('isNext (Predicted Value): ', True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
